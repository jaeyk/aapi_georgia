---
title: "01_web_scraping"
author: "Jae Yeon Kim"
output: html_document
---

# Install packages 

```{r}
if (!require(pacman)) install.packages("pacman")

pacman::p_load(tidyverse, 
               purrr, 
               rvest,
               readtext,
               webdriver,
               webshot,
               tm,
               vroom,
               here)

# install.packages("remotes")
remotes::install_github("rlesur/klippy")

# activate klippy
klippy::klippy()

# load custom functions 
source(here("functions", "text_clean.R"))
```

# Identity website elments 

The following code is adapted from here: https://slcladal.github.io/webcrawling.html

```{r}
# webshot::install_phantomjs()

# setup 
inst <- run_phantomjs()
session <- Session$new(port = inst$port)

# go to url 
session$go("https://www.advancingjustice-atlanta.org/newslettersarchieve")

# render page 
source <- session$getSource()

# html document
html_doc <- read_html(source)
```

```{r}
# text links 
links <- html_doc %>%
  html_nodes(xpath = "//div[@class='campaign']/a") %>%
  html_attr(name = "href")

# meta data 
meta <- html_doc %>%
  html_nodes(xpath = "//div[@class='campaign']") 

meta_out <- purrr::map(meta, possibly(meta2content, otherwise = "Error"))

meta_out <- meta_out %>%
  reduce(bind_rows)

# combine both of them
meta_out$link <- links
```

```{r}
texts <- purrr::map_dfr(meta_out$link, link2text)

names(meta_out) <- c("date", "title", "url")

combined <- left_join(meta_out, texts)
  
vroom::vroom_write(combined, file = here("processed_data", "aaaj_texts.csv"))
```