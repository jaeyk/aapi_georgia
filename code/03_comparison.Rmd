---
title: "Comparison"
author: "Jae Yeon Kim"
output: html_document
---

# Install packages

```{r}
options(max.print = 999999) # set an extreme value for the max print outputs 

if (!require(pacman)) install.packages("pacman")

pacman::p_load(tidyverse,
               lubridate,
               stringr, 
               zoo,
               purrr,
               readtext,
               textstem,
               textclean,
               tokenizers, # fast tool tokenizing
               DescTools, # descriptive tools
               stopwords,
               quanteda,
               tm,
               vroom,
               here,
               tidytext,
               tidymodels,
               glue,
               patchwork, 
               spacyr,
               pdftools,
               devtools,
               janitor, # alternative to base R table function 
               ggrepel) # add text to ggplots

# load custom functions
source(here("functions", "utils.R"))

# spacy_initialize(save_profile = TRUE)

#devtools::install_github("cbail/textnets")
#library(textnets)

#source(here("functions", "textnets.R"))
```

# Load data

```{r}
# AAAJ
aaaj <- vroom::vroom(here("processed_data", "aaaj_texts.csv"))

# 37% of the texts are missing 
mean(is.na(aaaj$msg))

# AAAF
aaaf <- read_csv(here("processed_data", "aaaf_texts.csv"))

# 7% of the texts are missing 
mean(is.na(aaaf$text))
```

# Bind data

```{r}
aaaj <- aaaj %>%
  rename(subject = title,
         text = msg)

df <- full_join(aaaj %>%
                  select(-url) %>%
                  mutate(source = "AAAJ-Atlanta"),
                aaaf %>%
                  mutate(source = "AAAF"))

df$processed_text <- clean_text(df$text)
df$processed_subject <- clean_text(df$subject)

df_copy <- df
```

# Describe data 

```{r}
mode_subjects <- df %>%
  group_by(source) %>%
  summarize(mode = Mode(processed_subject))
```

```{r}
# meta data 

## press release 
df$press_release <- if_else(str_detect(df$processed_subject, "press re"), 1, 0)

## recap
df$recap <- if_else(str_detect(df$processed_subject, "recap"), 1, 0)

df %>%
  group_by(source) %>%
  summarize(press_release_pct = mean(press_release),
            recap_pct = mean(recap))
```

# Wrangle data

```{r}
# add month variable
df <- df %>%
  mutate(month = as.yearmon(date, "%m/%Y"))
  
comb_n <- df %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "non_unique")

unique_comb_n <- df %>%
  distinct(date, processed_subject, source, press_release, recap) %>%
  mutate(month = as.yearmon(date, "%m/%Y")) %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "unique")
```

# Visualize monthly publication patterns

```{r}
aaaj_count <- comb_n %>%
  filter(str_detect(source, "AAAJ"))

aaaf_count <- comb_n %>%
  filter(str_detect(source, "AAAF"))

aaaj_count_plot <- aaaj_count %>%
  pivot_longer(cols = c("press_release_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "press_release_n" = "Press release",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 2) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 2) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[1]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaf_count_plot <- aaaf_count %>%
  pivot_longer(cols = c("recap_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "recap_n" = "Monthly recap",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 2) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 2) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[2]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaj_count_plot / aaaf_count_plot
  
ggsave(here("outputs", "pub_count.png"))
```

# Discover the most frequently mentioned locations over time across the two corpora

```{r}
# add meta data
raw_text <- df %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(text = clean_text(text)) %>%
#  distinct(source, text, year, date) %>%
  mutate(post_trump = if_else(date >= "2017-01-20", "Post-Trump", "Pre-Trump"))
```

```{r}
# extract entity
entity_text <- spacy_parse(raw_text$text, tag = FALSE, entity = TRUE, lemma = TRUE) %>% 
  entity_consolidate()
```

```{r}
# filter location entities
geos <- entity_text %>%
  filter(entity_type == "GPE")

# create a nice frequency table
geo_table <- tabyl(geos, lemma)

geo_table %>%
  mutate(county = if_else(str_detect(lemma, "county"), "County", "Non-county")) %>%
  group_by(county) %>%
  slice_max(order_by = percent, n = 5) %>%
  ggplot(aes(x = fct_reorder(lemma, percent), percent)) +
  geom_col() +
  facet_wrap(~county) +
  coord_flip() +
  labs(x = "", y = glue("Percent of the lemma among the geopolitical entities (n = {nrow(geo_table)})")) +
  scale_y_continuous(labels = function(x) glue("{x*100}%"))

ggsave(here("outputs", "geo_entity.png"))
```