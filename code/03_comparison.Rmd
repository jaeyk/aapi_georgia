---
title: "Comparison"
author: "Jae Yeon Kim"
output: html_document
---

# Install packages

```{r}
options(max.print = 999999) # set an extreme value for the max print outputs 

if (!require(pacman)) install.packages("pacman")

pacman::p_load(tidyverse,
               lubridate,
               stringr, 
               zoo,
               tidycensus,
               purrr,
               readtext,
               textstem,
               textclean,
               tokenizers, # fast tool tokenizing
               DescTools, # descriptive tools
               stopwords,
               quanteda,
               tm,
               vroom,
               here,
               tidytext,
               tidymodels,
               tidycensus, # to get the list of states and counties in the US
               glue,
               patchwork, 
               spacyr,
               pdftools,
               devtools,
               ggrepel,
               testthat, 
               textcat, 
               cld2, 
               cld3)

# load custom functions
source(here("functions", "utils.R"))

# spacy_initialize(save_profile = TRUE)

#devtools::install_github("cbail/textnets")
#library(textnets)

#source(here("functions", "textnets.R"))
```

# Load data

```{r}
# AAAJ
aaaj <- vroom::vroom(here("processed_data", "aaaj_texts.csv"))

# 37% of the texts are missing 
mean(is.na(aaaj$text))

# AAAF
aaaf <- read_csv(here("processed_data", "aaaf_texts.csv"))

# 8% of the texts are missing 
mean(is.na(aaaf$text))
```

# Bind data

```{r}
aaaj <- aaaj %>%
  rename(subject = title,
         text = msg)

df <- full_join(aaaj %>%
                  select(-url) %>%
                  mutate(source = "AAAJ-Atlanta"),
                aaaf %>%
                  mutate(source = "AAAF"))

df %>%
  group_by(source) %>%
  summarize(n = n())
```

```{r}
df$processed_text <- clean_text(df$text)
df$processed_subject <- clean_text(df$subject)

df_copy <- df
```

# Describe data 

```{r}
mode_subjects <- df %>%
  group_by(source) %>%
  summarize(mode = Mode(processed_subject))
```

```{r}
# meta data 

## press release 
df$press_release <- if_else(str_detect(df$processed_subject, "press re"), 1, 0)

## recap
df$recap <- if_else(str_detect(df$processed_subject, "recap"), 1, 0)

df %>%
  group_by(source) %>%
  summarize(press_release_pct = mean(press_release),
            recap_pct = mean(recap))
```

# Wrangle data

```{r}
# add month variable
df <- df %>%
  mutate(month = as.yearmon(date, "%m/%Y"))

df %>%
  group_by(source) %>%
  summarize(miss_pct = mean(is.na(text)))

df %>%
  group_by(source, month) %>%
  summarize(miss_pct = mean(is.na(text))) %>%
  ggplot(aes(x = month, y = miss_pct)) +
  geom_point() +
  facet_wrap(~source) +
  labs(y = "Missing rate",
       x = "Month")

ggsave(here("outputs", "missing_text.png"))

df %>%
  filter(is.na(text)) %>%
  group_by(source) %>%
  summarize(press_pct = mean(press_release),
            recap_pct = mean(recap))
```

```{r}
comb_n <- df %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "non_unique")

unique_comb_n <- df %>%
  distinct(date, processed_subject, source, press_release, recap) %>%
  mutate(month = as.yearmon(date, "%m/%Y")) %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "unique")
```

# Visualize monthly publication patterns

```{r}
aaaj_count <- comb_n %>%
  filter(str_detect(source, "AAAJ"))

aaaf_count <- comb_n %>%
  filter(str_detect(source, "AAAF"))

aaaj_count_plot <- aaaj_count %>%
  pivot_longer(cols = c("press_release_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "press_release_n" = "Press release",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[1]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaf_count_plot <- aaaf_count %>%
  pivot_longer(cols = c("recap_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "recap_n" = "Monthly recap",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[2]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaj_count_plot / aaaf_count_plot
  
ggsave(here("outputs", "pub_count.png"))
```

# Discover the most frequently mentioned locations over time across the two corpora

```{r}
# add meta data
raw_text <- df %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(text = clean_text(text)) %>%
#  distinct(source, text, year, date) %>%
  mutate(post_trump = if_else(date >= "2017-01-20", "Post-Trump", "Pre-Trump"))
```

```{r}
# split texts 
## AAAJ
aaaj_text <- raw_text %>%
  filter(str_detect(source, "AAAJ"))

## AAAF
aaaf_text <- raw_text %>%
  filter(str_detect(source, "AAAF"))

# extract entity
## AAAJ
aaaj_entity <- spacy_parse(aaaj_text$text, tag = FALSE, entity = TRUE, lemma = TRUE) %>% 
  entity_consolidate()

## AAAF
aaaf_entity <- spacy_parse(aaaf_text$text, tag = FALSE, entity = TRUE, lemma = TRUE) %>% 
  entity_consolidate()
```

```{r}
# filter location entities
## AAAJ
aaaj_geos <- aaaj_entity %>%
  filter(entity_type == "GPE")

## AAAF
aaaf_geos <- aaaf_entity %>%
  filter(entity_type == "GPE")
```

```{r}
## combine the two 
geos <- bind_rows(aaaj_geos %>%
  mutate(source = "AAAJ-Atlanta"),

aaaf_geos %>%
  mutate(source = "AAAF"))

test_that("the geos dataframe should include a column called source", {
  expect_equal(sum(names(geos) %in% c("source")) > 0, TRUE)}
)
```

```{r}
# add county column 
geos <- geos %>%
  mutate(county = if_else(str_detect(lemma, "county"), "County", "Non-county"))

# clean up counties 
recursive_clean <- function(x) {

  # hard coded example; I will comment it out later
  #x <- "gwinnett_county"
  
  message(paste("cleaning", x))
  
  # if there's a county in the element, clean up.
  # otherwise, leave it alone 
  x <- if_else(str_detect(x, "county"), str_extract(x, pattern = ".*(?=\\_)"), x)
  
  # if there's no _ or there's no "county" in the element then stop 
  if (!str_detect(x, "_") | !str_detect(x, "county")) {
      
      return(x)
    
  } else {
        
    # otherwise, keep going back to what's left 
    return(recursive_clean(x)) 
    
  }
  
}

# test the function
test_that("cleaning up the lemma vector", {
  expect_equal(recursive_clean("gwinnett_county"), "gwinnett")
}
)

# remove a non-English lemma (Korean) or a mailbox
geos <- geos %>%
  filter(lemma != "스페인어로_배포되는") %>%
  filter(!str_detect(lemma, "mailbox"))

new_lemma <- map_chr(geos$lemma, recursive_clean)

test_that("The row of the geos dataframe and the length of the new lemma should be equal if there's no miswrangling", {
          expect_equal(nrow(geos), length(new_lemma))
}
)

geos$clean_lemma <- new_lemma
```


```{r}
# create the frequency table via dplyr
geo_table <- geos %>%
  group_by(county, source, clean_lemma) %>%
  summarize(n = n()) %>%
  mutate(percent = n/sum(n)) %>%
  slice_max(order_by = percent, n = 5) # 10%
```

```{r}
geo_table %>%
  ggplot(aes(x = fct_reorder(clean_lemma, percent), percent, 
             fill = source)) +
    geom_col(position = position_dodge2()) +
    coord_flip() +
    labs(x = "Named entities", y = "Proportion among the geopolitical entities (%)",
         fill = "Source") +
    scale_y_continuous(labels = function(x) glue("{x*100}%")) +
    facet_wrap(~county)

ggsave(here("outputs", "geo_entity.png"))
```

# Dictionary analysis

```{r}
ga_counties <- tidycensus::fips_codes %>%
  filter(state == "GA") %>%
  mutate(county = str_replace_all(county, "County", "")) %>%
  mutate(county = str_trim(county),
         county = tolower(county)) %>%
  pull(county)

count_county <- function(x) {

  # 67 = gwinnett 
  var <- ga_counties[x]
    
  count <- ifelse(str_detect(df$processed_text, var), str_count(df$processed_text, pattern = var), 0)
  
  out <- data.frame(count)
  
  names(out) <- var
  
  return(out)
  
}

ga_counties_count <- purrr::map_dfc(seq(ga_counties), count_county)
```

```{r}
county_freq <- df %>%
  select(date, subject, source) %>%
  bind_cols(ga_counties_count)

write_csv(county_freq, here("processed_data", "county_freq.csv"))
```

```{r}
dic_time_scores <- county_freq %>%
  pivot_longer(cols = -c("date", "subject", "source"), 
               names_to = "counties", 
               values_to = "dummy_scores")

dic_time_scores <- dic_time_scores %>%
  mutate(month = as.yearmon(date, "%m/%Y"))

county_sep <- dic_time_scores %>%
  group_by(counties) %>%
  summarize(total_word_count = sum(dummy_scores, na.rm = T)) %>%
  arrange(desc(total_word_count)) %>%
  mutate(zero_county = if_else(total_word_count == 0, "zero", "non-zero"))

dic_viz_df <- dic_time_scores %>% 
  left_join(county_sep)

dic_viz_df %>%
  filter(zero_county == "non-zero") %>%
  pull(counties) %>%
  unique() %>%
  length() # 66 counties

66/length(ga_counties) # 41%
```

```{r}
dic_viz <- dic_viz_df %>%
  filter(zero_county == "non-zero") %>%
  group_by(source, counties, date) %>%
  summarize(total_daily_scores = sum(dummy_scores, na.rm = T)) %>%
  mutate(counties = factor(counties)) %>%
  mutate(counties = fct_reorder(counties, total_daily_scores)) 
```

```{r}
dic_viz %>%
  ggplot(aes(x = as.Date(date), y = total_daily_scores, col = source)) +
  geom_line() +
  labs(x = "Date", y = "Daily word count",
       col = "Source") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27")) +
  facet_wrap(~counties, ncol = 6)

ggsave(here("outputs", "geo_overtime.png"), 
       width = 10,
       height = 10)
```