---
title: "Comparison"
author: "Jae Yeon Kim"
output: html_document
---

# Install packages

```{r}
options(max.print = 999999) # set an extreme value for the max print outputs 

if (!require(pacman)) install.packages("pacman")

pacman::p_load(tidyverse,
               lubridate,
               stringr, 
               zoo,
               tidycensus,
               purrr,
               readtext,
               textstem,
               textclean,
               tokenizers, # fast tool tokenizing
               DescTools, # descriptive tools
               stopwords,
               quanteda,
               tm,
               vroom,
               here,
               tidytext,
               tidymodels,
               glue,
               patchwork, 
               spacyr,
               pdftools,
               devtools,
               ggrepel,
               testthat, 
               textcat, 
               cld2, 
               cld3)

# load custom functions
source(here("functions", "utils.R"))

# spacy_initialize(save_profile = TRUE)

#devtools::install_github("cbail/textnets")
#library(textnets)

#source(here("functions", "textnets.R"))
```

# Load data

```{r}
# AAAJ
aaaj <- vroom::vroom(here("processed_data", "aaaj_texts.csv"))

# 37% of the texts are missing 
mean(is.na(aaaj$msg))

# AAAF
aaaf <- read_csv(here("processed_data", "aaaf_texts.csv"))

# 7% of the texts are missing 
mean(is.na(aaaf$text))
```

# Bind data

```{r}
aaaj <- aaaj %>%
  rename(subject = title,
         text = msg)

df <- full_join(aaaj %>%
                  select(-url) %>%
                  mutate(source = "AAAJ-Atlanta"),
                aaaf %>%
                  mutate(source = "AAAF"))

df$processed_text <- clean_text(df$text)
df$processed_subject <- clean_text(df$subject)

df_copy <- df
```

# Describe data 

```{r}
mode_subjects <- df %>%
  group_by(source) %>%
  summarize(mode = Mode(processed_subject))
```

```{r}
# meta data 

## press release 
df$press_release <- if_else(str_detect(df$processed_subject, "press re"), 1, 0)

## recap
df$recap <- if_else(str_detect(df$processed_subject, "recap"), 1, 0)

df %>%
  group_by(source) %>%
  summarize(press_release_pct = mean(press_release),
            recap_pct = mean(recap))
```

# Wrangle data

```{r}
# add month variable
df <- df %>%
  mutate(month = as.yearmon(date, "%m/%Y"))
  
comb_n <- df %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "non_unique")

unique_comb_n <- df %>%
  distinct(date, processed_subject, source, press_release, recap) %>%
  mutate(month = as.yearmon(date, "%m/%Y")) %>%
  group_by(month, source) %>%
  summarize(n = n(),
            press_release_n = sum(press_release),
            recap_n = sum(recap)) %>%
  mutate(type = "unique")
```

# Visualize monthly publication patterns

```{r}
aaaj_count <- comb_n %>%
  filter(str_detect(source, "AAAJ"))

aaaf_count <- comb_n %>%
  filter(str_detect(source, "AAAF"))

aaaj_count_plot <- aaaj_count %>%
  pivot_longer(cols = c("press_release_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "press_release_n" = "Press release",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[1]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaf_count_plot <- aaaf_count %>%
  pivot_longer(cols = c("recap_n", "n"),
               values_to = "scores", 
               names_to = "variables") %>%
  mutate(variables = recode(variables, 
                            "recap_n" = "Monthly recap",
                            "n" = "Total"
                               )) %>%
  ggplot(aes(x = as.Date(month), y = scores, col = variables)) +
  geom_line(alpha = 0.8) +
  labs(x = "Year Month", y = "") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  labs(title = glue("Monthly {unique(comb_n$source)[2]} publication count, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "",
       subtitle = "Dotted redlines: U.S. Presidential elections",
       col = "N") +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

aaaj_count_plot / aaaf_count_plot
  
ggsave(here("outputs", "pub_count.png"))
```

# Discover the most frequently mentioned locations over time across the two corpora

```{r}
# add meta data
raw_text <- df %>%
  mutate(year = lubridate::year(date)) %>%
  mutate(text = clean_text(text)) %>%
#  distinct(source, text, year, date) %>%
  mutate(post_trump = if_else(date >= "2017-01-20", "Post-Trump", "Pre-Trump"))
```

```{r}
# split texts 
## AAAJ
aaaj_text <- raw_text %>%
  filter(str_detect(source, "AAAJ"))

## AAAF
aaaf_text <- raw_text %>%
  filter(str_detect(source, "AAAF"))

# extract entity
## AAAJ
aaaj_entity <- spacy_parse(aaaj_text$text, tag = FALSE, entity = TRUE, lemma = TRUE) %>% 
  entity_consolidate()

## AAAF
aaaf_entity <- spacy_parse(aaaf_text$text, tag = FALSE, entity = TRUE, lemma = TRUE) %>% 
  entity_consolidate()
```

```{r}
# filter location entities
## AAAJ
aaaj_geos <- aaaj_entity %>%
  filter(entity_type == "GPE")

## AAAF
aaaf_geos <- aaaf_entity %>%
  filter(entity_type == "GPE")
```

```{r}
## combine the two 
geos <- bind_rows(aaaj_geos %>%
  mutate(source = "AAAJ-Atlanta"),

aaaf_geos %>%
  mutate(source = "AAAF"))

test_that("the geos dataframe should include a column called source", {
  expect_equal(sum(names(geos) %in% c("source")) > 0, TRUE)}
)
```

```{r}
# add county column 
geos <- geos %>%
  mutate(county = if_else(str_detect(lemma, "county"), "County", "Non-county"))

# clean up counties 
recursive_clean <- function(x) {

  # hard coded example; I will comment it out later
  #x <- "gwinnett_county"
  
  message(paste("cleaning", x))
  
  # if there's a county in the element, clean up.
  # otherwise, leave it alone 
  x <- if_else(str_detect(x, "county"), str_extract(x, pattern = ".*(?=\\_)"), x)
  
  # if there's no _ or there's no "county" in the element then stop 
  if (!str_detect(x, "_") | !str_detect(x, "county")) {
      
      return(x)
    
  } else {
        
    # otherwise, keep going back to what's left 
    return(recursive_clean(x)) 
    
  }
  
}

# test the function
test_that("cleaning up the lemma vector", {
  expect_equal(recursive_clean("gwinnett_county"), "gwinnett")
}
)

# remove a non-English lemma (Korean) or a mailbox
geos <- geos %>%
  filter(lemma != "스페인어로_배포되는") %>%
  filter(!str_detect(lemma, "mailbox"))

new_lemma <- map_chr(geos$lemma, recursive_clean)

test_that("The row of the geos dataframe and the length of the new lemma should be equal if there's no miswrangling", {
          expect_equal(nrow(geos), length(new_lemma))
}
)

geos$clean_lemma <- new_lemma
```


```{r}
# create the frequency table via dplyr
geo_table <- geos %>%
  group_by(county, source, clean_lemma) %>%
  summarize(n = n()) %>%
  mutate(percent = n/sum(n)) %>%
  slice_max(order_by = percent, n = 5) # 10%
```

```{r}
geo_table %>%
  ggplot(aes(x = fct_reorder(clean_lemma, percent), percent, 
             fill = source)) +
    geom_col(position = position_dodge2()) +
    coord_flip() +
    labs(x = "Named entities", y = "Proportion among the geopolitical entities (%)",
         fill = "Source") +
    scale_y_continuous(labels = function(x) glue("{x*100}%")) +
    facet_wrap(~county)

ggsave(here("outputs", "geo_entity.png"))
```

# Dictionary analysis

```{r}
aapi_counties <- c("gwinnett", "forsyth", "fulton", "cobb", "dekalb", "columbia", "clayton", "barrow", "fayette", "houston", "henry")

i <- 1:length(aapi_counties)

# generate slightly repetitive code
glue("{aapi_counties[i]} = ifelse(str_detect(processed_text, '{aapi_counties[i]}'), str_count(processed_text), 0))") 
           
df <- df %>%
  mutate(gwinnett = ifelse(str_detect(processed_text, 'gwinnett'), str_count(processed_text), 0),
        forsyth = ifelse(str_detect(processed_text, 'forsyth'), str_count(processed_text), 0),
        fulton = ifelse(str_detect(processed_text, 'fulton'), str_count(processed_text), 0),
        cobb = ifelse(str_detect(processed_text, 'cobb'), str_count(processed_text), 0),
        dekalb = ifelse(str_detect(processed_text, 'dekalb'), str_count(processed_text), 0),
        columbia = ifelse(str_detect(processed_text, 'columbia'), str_count(processed_text), 0),
        clayton = ifelse(str_detect(processed_text, 'clayton'), str_count(processed_text), 0),
        barrow = ifelse(str_detect(processed_text, 'barrow'), str_count(processed_text), 0),
        fayette = ifelse(str_detect(processed_text, 'fayette'), str_count(processed_text), 0),
        houston = ifelse(str_detect(processed_text, 'houston'), str_count(processed_text), 0),
        henry = ifelse(str_detect(processed_text, 'henry'), str_count(processed_text), 0))

dic_time_scores <- df %>%
  select(-c("text", "processed_text")) %>%
  pivot_longer(cols = aapi_counties, 
               names_to = "counties", 
               values_to = "dummy_scores")

dic_time_scores <- dic_time_scores %>%
  mutate(month = as.yearmon(date, "%m/%Y"))

dic_time_scores %>%
  group_by(source, counties, date) %>%
  summarize(total_daily_scores = sum(dummy_scores, na.rm = T)) %>%
  mutate(counties = factor(counties)) %>%
  mutate(counties = fct_reorder(counties, total_daily_scores)) %>%
  ggplot(aes(x = as.Date(date), y = total_daily_scores, col = source)) +
  geom_line() +
  labs(x = "Date", y = "Daily word count",
       col = "Source") +
  facet_wrap(~counties) +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red",
             linetype = "dotted",
             size = 1) +
  theme_bw() +
  scale_colour_viridis_d(begin = 0.2, end = 0.5) +
  xlim(as.Date("2012-01-01"), as.Date("2022-01-27"))

ggsave(here("outputs", "geo_overtime.png"), width = 12)
```