---
title: "01_web_scraping"
author: "Jae Yeon Kim"
output: html_document
---

# Install packages 

```{r}
if (!require(pacman)) install.packages("pacman")

pacman::p_load(tidyverse, 
               lubridate,
               zoo,
               purrr, 
               tm.plugin.mail,
               readtext,
               tm,
               vroom,
               here, 
               tidytext, 
               tidymodels,
               glue,
               spacyr, 
               pdftools)

# load custom functions 
source(here("functions", "utils.R"))

spacy_initialize(save_profile = TRUE)
```

# Load files 

The code was adapted from here: https://stackoverflow.com/questions/32879496/import-and-parse-eml-files

```{r}
mbf <- here("raw_data", "aaaf", "newslettters")

mailfiles <- list.files(mbf, full.names=TRUE)

pdf_files <- mailfiles[str_detect(mailfiles, ".pdf")]

length(pdf_files)
```

# Parse texts 

```{r}
df <- map_dfr(seq(length(pdf_files)), parse_pdf)

write_csv(df, here("processed_data", "aaaf_texts.csv"))
```

# Identity website elments 

The following code is adapted from here: https://slcladal.github.io/webcrawling.html

```{r}
# webshot::install_phantomjs()

# setup 
inst <- run_phantomjs()
session <- Session$new(port = inst$port)

# go to url 
session$go("https://www.advancingjustice-atlanta.org/newslettersarchieve")

# render page 
source <- session$getSource()

# html document
html_doc <- read_html(source)
```

```{r}
# text links 
links <- html_doc %>%
  html_nodes(xpath = "//div[@class='campaign']/a") %>%
  html_attr(name = "href")

# meta data 
meta <- html_doc %>%
  html_nodes(xpath = "//div[@class='campaign']") 

meta_out <- purrr::map(meta, possibly(meta2content, otherwise = "Error"))

meta_out <- meta_out %>%
  reduce(bind_rows)

# combine both of them
meta_out$link <- links
```

# Extract the texts from the website 

```{r}
texts <- purrr::map_dfr(meta_out$link, link2text)

names(meta_out) <- c("date", "title", "url")

combined <- left_join(meta_out, texts)
  
vroom::vroom_write(combined, file = here("processed_data", "aaaj_texts.csv"))
```

# Exploratoray data analysis (data visualization)

```{r}
combined <- vroom::vroom(here("processed_data", "aaaj_texts.csv"))
```

```{r}
comb_n <- combined %>%
  mutate(month = as.yearmon(date, "%Y %m")) %>%
#  distinct(month, title, msg, url) %>%
  group_by(month) %>%
  count() 

comb_n %>%
  ggplot(aes(x = as.Date(month), y = n)) +
  geom_point() +
  geom_line(alpha = 0.3) +
  labs(x = "Date", y = "Count") +
  geom_vline(xintercept = as.Date("2016-11-08"),
             col = "red", 
             linetype = "dotted") +
  geom_vline(xintercept = as.Date("2020-11-03"),
             col = "red", 
             linetype = "dotted") +
  geom_rect(aes(xmin = as.Date("2016-11-08"), xmax = as.Date("2020-11-03"), 
                ymin = -Inf, ymax = +Inf, 
                fill = "2016-2022 presidential election cycle"),
                inherit.aes = FALSE,
                alpha = 0.01) +
  theme_bw() +
  labs(title = "Asian Americans for Advancing Justice - Atlanta",
       subtitle = glue("Newsletter Publication Pattern, {min(year(comb_n$month))}-{max(year(comb_n$month))}"),
       x = "") +
  guides(fill="none")

ggsave(here("outputs", "aaaj_pub_desc.png"))
```